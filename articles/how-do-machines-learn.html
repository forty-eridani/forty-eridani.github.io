<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Home</title>
    <link rel="icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="stylesheet" href="/css/common.css" />
    <link rel="stylesheet" href="/css/articles/how-machines-learn.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Merriweather:ital,opsz,wght@0,18..144,300..900;1,18..144,300..900&family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
      rel="stylesheet"
    />

    <script
      type="text/javascript"
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
  </head>
  <body>
    <div class="navbar">
      <div class="nav-container-left">
        <a href="/" class="navigation">Nolan Thwaits</a>
      </div>
      <div class="nav-container-right">
        <a href="/about.html" class="navigation">About</a>
        <a href="/contact.html" class="navigation">Contact</a>
      </div>
    </div>

    <div class="page-title">
      <h1 class="">How Do Machines Learn?</h1>
      <hr class="line" />
    </div>

    <div class="section"></div>

    <div class="page-content">
      <h2 class="section-header">Introduction</h2>
      <p class="paragraph">
        In this era, the use of machine learning seems to be extremely
        commonplace. Everything from targeted advertisements to stock market
        predictions to speech recognition all typically use some form of machine
        learning, often in the form of neural networks. Despite their
        prevalence, most of those who are subject to such algorithms lack the
        fundamental knowledge of how they work. In this (rather long) article,
        we will go beyond a typical explanation of a neural network and will
        instead dive deep into the math and computer science that underlies
        their function. By the end of this article, we will have developed a
        fully functional program (written in python) that can identify digits
        from the MNIST database. The MNIST database (short for Modified National
        Institute of Technology) is a collection of handwritten digits with
        labels commonly used to train systems just like the one we will create.
        Our goal will to create a program that can accurately identify a large
        majority of these digits without any special python libraries (besides
        numpy, no way in hell am I writing vector and matrix operations when
        numpy exists).
      </p>
      <br />
      <p class="paragraph">
        This article is not for the faint of heart. Some amount of programming
        knowledge will be necessary to not only follow but understand the
        program that we will write. I highly encourage anybody reading to
        re-read any section they don't understand as well as looking into other
        resources if there are any parts I did not articulate well. If you find
        that I could have explained something better or made some sort of error
        or have some other suggestion, please feel free to reach out as I would
        be happy to edit this resource. The math for this may become somewhat
        complex. While we will be touching on a slight amount of multi-variable
        calculus, I will do my best to explain the concepts as intuitively as
        possible and avoid a lot of the notation that a lot of people get
        trapped in while learning this topic, at least until we have covered
        enough where the notation can be understood with the intuition I have
        provided. If at any point the math feels like too much, I highly
        recommend checking out out 3Blue1Brown's calculus or linear algebra
        courses. They have incredible animations paired with great explanations
        on the math topics we will cover. 3Blue1Brown also has a course on
        neural networks, so that may also be a useful resource if you ever feel
        stuck. I gained much of my knowledge on the topic from 3Blue1Brown as
        well as Michael Nielson's book
        <a href="https://neuralnetworksanddeeplearning.com/" class="hyperlink"
          >Neural Networks and Deep Learning</a
        >. Anywho, enough of the blabbering, let's get into the fun stuff!
      </p>
      <br />
      <h2 class="section-header">Perceptrons and Decision Boundaries</h2>
      <p class="paragraph">
        The perceptron is the the simplest neural network that is out there and
        works in a similar way to how you probably weigh your own decisions. Let
        us imagine that you are deciding whether to learn the piccolo. We can
        represent the decision-making process by quantifying the factors that go
        into that decision. For now, let's consider just two factors that may
        influence your decision (this will be easier to visually represent
        later): the amount of hours per week you would put into the instrument,
        and how much you enjoy the piccolo.<sup
          ><a href="#fn1" id="tfn1" class="footnote-navigation">1</a></sup
        >
        We can represent these factors as the "activation" of two input neurons,
        and they both lead into another neuron known as the output neuron. The
        output nueron will sum all of the input activations and apply a
        threshold that represents whether you are actually willing to learn how
        to play the piccolo with the given parameters.
      </p>
      <div class="image-container">
        <object
          data="/media/images/simple-perceptron.svg"
          type=""
          width="250px"
          height="250px"
        ></object>
      </div>
      <br />
      <p class="paragraph">
        For now, let's just arbitrarily consider the threshold as zero. We can
        represent possible activations for our inputs on a coordinate plane, and
        the possible output activation (whether we actually want to learn the
        piccolo) will be split by a line since our output activation would be x
        (enjoyment) + y (practice time) > 0 (the blue region represents the
        combinations of factors that would lead to us making the decision to
        play the piccolo).
      </p>

      <br />

      <div class="image-container">
        <object
          data="/media/images/untransformed-decision-bounary.svg"
          type=""
          class="image"
        ></object>
      </div>

      <br />

      <p class="paragraph">
        As you could probably surmise, this doesn't really represent how we feel
        about playing the piccolo. The more we practice, the less enjoyment is
        required for us to decide that we would like to play the piccolo!
        <sup><a href="#fn2" class="footnote-navigation" id="tfn2">2</a></sup>
        This is where weights and biases come in. We need some way to transform
        those input parameters in a way that could represent our decision
        process. In other words, we must simply weigh the factors that go into
        our decision. Weights in a neural network are essentially seeing how
        important an activation is to the output by applying some coefficient to
        it's own activation before the activation is sent to the output.
      </p>

      <div class="image-container">
        <object data="/media/images/weight-network.svg" type=""></object>
      </div>

      <p class="paragraph">
        For our example, we would probably consider practice time a negative to
        our decision to play piccolo. To show this within our perceptron, we
        could imagine the input neuron representing time receiving a negative
        weight. This would mean that the more time we would practice, the more
        we would have to enjoy the instrument in order for us to decide to play
        the piccolo. Up until now, all of our weights have been 1 since we are
        directly piping in the activations of the inputs neurons straight into
        the output neurons, but to get a more accurate representation of our
        decision, let's just say the the weight between practice time and the
        output is -1.
      </p>

      <br />

      <div class="image-container">
        <object
          data="/media/images/transformed-decision-boundary.svg"
          type=""
        ></object>
      </div>

      <br />

      <p class="paragraph">
        Now that's more like it! However, there is one more thing we must
        consider: bias. Bias is essentially the offset we apply to the sum of
        the activations before they reach the output. According to the graph we
        have just created, even if we like the piccolo just a tiny amount (to
        put this another way, \( x = \epsilon \) where \( \epsilon > 0 \)), we
        are willing to learn it with some minimal amount of practice. Now maybe
        that is you, but to learn the piccolo there is an upfront cost which
        most wouldn't be willing to pay if they only liked the instrument a
        small amount. To represent this numerically, we use the aforementioned
        bias. In this case, the negative bias will simply act as the minimum
        enjoyment it will take to make the decision to get an instrument with
        some small amount of practice.<sup
          ><a href="#fn3" class="footnote-navigation" id="tfn3">3</a></sup
        >
        Let's say the minimum enjoyment is two units which would mean a bias of
        -2.
      </p>

      <br />

      <div class="image-container">
        <object
          data="/media/images/transformed-biased-decision-boundary.svg"
          type=""
        ></object>
      </div>

      <br />

      <p class="paragraph">
        Now here we have it! Our decision making plot to decide whether we
        should play piccolo. However, there is an aspect to this I have
        neglected to mention much about: the decision boundary. The decision
        boundary is a fundamental aspect to any algorithm that classifies
        information like we are. As the name implies, it is the boundary between
        regions of classification. In our case, this is simply the line that
        separates whether we should play the piccolo or not with the given
        inputs, but as the classification becomes more abstract (like
        recognizing the digits from the MNIST database), so too do the
        boundaries. In the case of the MNIST dataset, there are 784 input
        parameters, and if we were to represent that like how we have
        represented our piccolo antics, it would require a visualization of 784
        dimensions! This means our decision boundaries are hyperplanes
        (basically 3d planes in more dimensions). Unfortunately, us mere mortals
        are unable to visualize such a high-dimensions space, but if you can
        grasp the idea of 2d or 3d decision boundaries, just understand that the
        math for higher dimensions all works out the same.
      </p>

      <br />

      <p class="paragraph">
        Now that we grasp the intuition, let's put our activation in proper
        mathematical terms. If we are just keeping with our example of a simple
        perceptron, we could simply sum over all of the inputs and add the bias.
        In math-speak, this would be \( a = \sum_{i=1}^n (w^i \cdot x^i) + b \)
        where \( a \) is the activation of the output neuron, \( n \) is the
        amount of input neurons, \( w^i \) is the weight at index \( i \), and
        \( x^i \) is the input activation at neuron index \( i \). If you are
        not too comfortable with summation notation, I would recommend
        familiarizing yourself with it as we will be using it a lot throughout
        this article.
      </p>

      <br />

      <div class="footnotes">
        <hr class="line" />
        <sup id="fn1">
          1. Most introductions to perceptrons contain contain only binary input
          neurons (as in they are either on or off) and then apply the weights.
          I am taking a slightly different approach to help motivate a more
          geometric interpretation of how neural networks truly function since
          it will soon become much more complex.
          <a href="#tfn1" title="Return to text" class="footnote-navigation"
            >↩</a
          ></sup
        >
        <sup id="fn2">
          2. Pretend that our practice time could be negative. I am unsure how
          that is really possible, maybe intentionally worsening our skills at
          piccolo somehow? I am too lazy to redo and re-edit the GeoGebra svg
          file, and this gives us a more holistic view of the possible
          activations of other kinds of parameters beyond perfect piccolo
          predictions.
          <a href="#tfn2" title="Return to text" class="footnote-navigation"
            >↩</a
          ></sup
        >
        <sup id="fn3">
          3. Okay well technically the bias will not be the minimum enjoyment
          since at x = -bias the practice time will be zero as the terms cancel
          out, but bear with me.
          <a href="#tfn3" title="Return to text" class="footnote-navigation"
            >↩</a
          ></sup
        >
      </div>
    </div>
    <div class="bottom"></div>
  </body>
</html>
