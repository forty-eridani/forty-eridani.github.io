<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Home</title>
    <link rel="icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="stylesheet" href="/css/common.css" />
    <link rel="stylesheet" href="/css/articles/how-machines-learn.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&family=Merriweather:ital,opsz,wght@0,18..144,300..900;1,18..144,300..900&family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
      rel="stylesheet"
    />

    <script
      type="text/javascript"
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
  </head>
  <body>
    <div class="navbar">
      <div class="nav-container-left">
        <a href="/" class="navigation">Nolan Thwaits</a>
      </div>
      <div class="nav-container-right">
        <a href="/about.html" class="navigation">About</a>
        <a href="/contact.html" class="navigation">Contact</a>
      </div>
    </div>

    <div class="page-title">
      <h1 class="">How Do Machines Learn?</h1>
      <hr class="line" />
    </div>

    <div class="section"></div>

    <div class="page-content">
      <h2 class="section-header">Introduction</h2>
      <p class="paragraph">
        In this era, the use of machine learning seems to be extremely
        commonplace. Everything from targeted advertisements to stock market
        predictions to speech recognition all typically use some form of machine
        learning, often in the form of neural networks. Despite their
        prevalence, most of those who are subject to such algorithms lack the
        fundamental knowledge of how they work. In this (rather long) article,
        we will go beyond a typical explanation of a neural network and will
        instead dive deep into the math and computer science that underlies
        their function. By the end of this article, we will have developed a
        fully functional program (written in python) that can identify digits
        from the MNIST database. The MNIST database (short for Modified National
        Institute of Standards and Technology) is a collection of handwritten
        digits with labels commonly used to train systems just like the one we
        will create. Our goal will to create a program that can accurately
        identify a large majority of these digits without any special python
        libraries (besides numpy, no way in hell am I writing vector and matrix
        operations when numpy exists).
      </p>

      <p class="paragraph">
        This article is not for the faint of heart. Some amount of programming
        knowledge will be necessary to not only follow but understand the
        program that we will write. I highly encourage anybody reading to
        re-read any section they don't understand as well as looking into other
        resources if there are any parts I did not articulate well. If you find
        that I could have explained something better or made some sort of error
        or have some other suggestion, please feel free to reach out as I would
        be happy to edit this resource. The math for this may become somewhat
        complex. While we will be touching on a slight amount of multi-variable
        calculus, I will do my best to explain the concepts as intuitively as
        possible and avoid a lot of the notation that a lot of people get
        trapped in while learning this topic, at least until we have covered
        enough where the notation can be understood with the intuition I have
        provided. If at any point the math feels like too much, I highly
        recommend checking out out 3Blue1Brown's calculus or linear algebra
        courses. They have incredible animations paired with great explanations
        on the math topics we will cover. 3Blue1Brown also has a course on
        neural networks, so that may also be a useful resource if you ever feel
        stuck. I gained much of my knowledge on the topic from 3Blue1Brown as
        well as Michael Nielson's book
        <a href="https://neuralnetworksanddeeplearning.com/" class="hyperlink"
          >Neural Networks and Deep Learning</a
        >. Anywho, enough of the blabbering, let's get into the fun stuff!
      </p>
      <br />
      <h2 class="section-header">Perceptrons and Decision Boundaries</h2>
      <p class="paragraph">
        The perceptron is the the simplest neural network that is out there and
        works in a similar way to how you probably weigh your own decisions. Let
        us imagine that you are deciding whether to learn the piccolo. We can
        represent the decision-making process by quantifying the factors that go
        into that decision. For now, let's consider just two factors that may
        influence your decision (this will be easier to visually represent
        later): the amount of hours per week you would put into the instrument,
        and how much you enjoy the piccolo.<sup
          ><a href="#fn1" id="tfn1" class="footnote-navigation">1</a></sup
        >
        We can represent these factors as the "activation" of two input neurons,
        and they both lead into another neuron known as the output neuron. The
        output neuron will sum all of the input activations and apply a
        threshold that represents whether you are actually willing to learn how
        to play the piccolo with the given parameters.
      </p>

      <div class="image-container">
        <object
          data="/media/images/simple-perceptron.svg"
          type=""
          width="250px"
          height="250px"
        ></object>
      </div>

      <p class="paragraph">
        For now, let's just arbitrarily consider the threshold as zero. We can
        represent possible activations for our inputs on a coordinate plane, and
        the possible output activation (whether we actually want to learn the
        piccolo) will be split by a line since our output activation would be x
        (enjoyment) + y (practice time) > 0 (the blue region represents the
        combinations of factors that would lead to us making the decision to
        play the piccolo).
      </p>

      <br />

      <div class="image-container">
        <object
          data="/media/images/untransformed-decision-bounary.svg"
          type=""
          class="image"
        ></object>
      </div>

      <br />

      <p class="paragraph">
        As you could probably surmise, this doesn't really represent how we feel
        about playing the piccolo. The more we practice, the less enjoyment is
        required for us to decide that we would like to play the piccolo!
        <sup><a href="#fn2" class="footnote-navigation" id="tfn2">2</a></sup>
        This is where weights and biases come in. We need some way to transform
        those input parameters in a way that could represent our decision
        process. In other words, we must simply weigh the factors that go into
        our decision. Weights in a neural network are essentially seeing how
        important an activation is to the output by applying some coefficient to
        it's own activation before the activation is sent to the output.
      </p>

      <div class="image-container">
        <object data="/media/images/weight-network.svg" type=""></object>
      </div>

      <p class="paragraph">
        For our example, we would probably consider practice time a negative to
        our decision to play piccolo. To show this within our perceptron, we
        could imagine the input neuron representing time receiving a negative
        weight. This would mean that the more time we would practice, the more
        we would have to enjoy the instrument in order for us to decide to play
        the piccolo. Up until now, all of our weights have been 1 since we are
        directly piping in the activations of the inputs neurons straight into
        the output neurons, but to get a more accurate representation of our
        decision, let's just say the the weight between practice time and the
        output is -1.
      </p>

      <br />

      <div class="image-container">
        <object
          data="/media/images/transformed-decision-boundary.svg"
          type=""
        ></object>
      </div>

      <br />

      <p class="paragraph">
        Now that's more like it! However, there is one more thing we must
        consider: bias. Bias is essentially the offset we apply to the sum of
        the activations before they reach the output. According to the graph we
        have just created, even if we like the piccolo just a tiny amount (to
        put this another way, \( x = \epsilon \) for some small \( \epsilon > 0
        \)), we are willing to learn it with some minimal amount of practice.
        Now maybe that is you, but to learn the piccolo there is an upfront cost
        which most wouldn't be willing to pay if they only liked the instrument
        a small amount. To represent this numerically, we use the aforementioned
        bias. In this case, the negative bias will simply act as the minimum
        enjoyment it will take to make the decision to get an instrument with
        some small amount of practice.<sup
          ><a href="#fn3" class="footnote-navigation" id="tfn3">3</a></sup
        >
        Let's say the minimum enjoyment is two units which would mean a bias of
        -2.
      </p>

      <br />

      <div class="image-container">
        <object
          data="/media/images/transformed-biased-decision-boundary.svg"
          type=""
        ></object>
      </div>

      <br />

      <p class="paragraph">
        Now here we have it! Our decision making plot to decide whether we
        should play piccolo. However, there is an aspect to this I have
        neglected to mention much about: the decision boundary. The decision
        boundary is a fundamental aspect to any algorithm that classifies
        information like we are. As the name implies, it is the boundary between
        regions of classification. In our case, this is simply the line that
        separates whether we should play the piccolo or not with the given
        inputs, but as the classification becomes more abstract (like
        recognizing the digits from the MNIST database), so too do the
        boundaries. In the case of the MNIST dataset, there are 784 input
        parameters, and if we were to represent that like how we have
        represented our piccolo antics, it would require a visualization of 784
        dimensions! This means our decision boundaries are hyperplanes
        (basically 3d planes in more dimensions). Unfortunately, us mere mortals
        are unable to visualize such a high-dimensions space, but if you can
        grasp the idea of 2d or 3d decision boundaries, just understand that the
        math for higher dimensions all works out the same.
      </p>

      <p class="paragraph">
        Now that we grasp the intuition, let's put our activation in proper
        mathematical terms. If we are just keeping with our example of a simple
        perceptron, we could simply sum over all of the inputs and add the bias.
        In math-speak, this would be \( a = \sum_{i=1}^n (w^i \cdot x^i) + b > t
        \) where \( a \) is the activation of the output neuron, \( n \) is the
        amount of input neurons, \( w^i \) is the weight at index \( i \), \(
        x^i \) is the input activation at neuron index \( i \), and \( t\) is
        our threshold. If you are not too comfortable with summation notation, I
        would recommend familiarizing yourself with it as we will be using it a
        lot throughout this article.
      </p>

      <br />

      <div class="footnotes">
        <hr class="line" />
        <sup id="fn1">
          1. Most introductions to perceptrons contain contain only binary input
          neurons (as in they are either on or off) and then apply the weights.
          I am taking a slightly different approach to help motivate a more
          geometric interpretation of how neural networks truly function since
          it will soon become much more complex.
          <a href="#tfn1" title="Return to text" class="footnote-navigation"
            >Return to text</a
          ></sup
        >
        <sup id="fn2">
          2. Pretend that our practice time could be negative. I am unsure how
          that is really possible, maybe intentionally worsening our skills at
          piccolo somehow? I am too lazy to redo and re-edit the GeoGebra svg
          file, and this gives us a more holistic view of the possible
          activations of other kinds of parameters beyond perfect piccolo
          predictions.
          <a href="#tfn2" title="Return to text" class="footnote-navigation"
            >Return to text</a
          ></sup
        >
        <sup id="fn3">
          3. Okay well technically the bias will not be the minimum enjoyment
          since at x = -bias the practice time will be zero as the terms cancel
          out, but bear with me.
          <a href="#tfn3" title="Return to text" class="footnote-navigation"
            >Return to text</a
          ></sup
        >
      </div>

      <br />

      <h2>Hidden layers and Feed-Forward</h2>

      <br />

      <p class="paragraph">
        Our perceptron from the previous chapter is extremely simple without any
        significantly large operations performed to the inputs. Most useful
        neural networks require hidden layers. A hidden layer can be thought of
        in many ways, but to stick to our geometric view let's imagine them as
        matrix transformations. Because we have not yet learned of
        non-polynomial activation functions, hidden layers will not serve any
        meaningful purpose for us since all of the linear transformations in a
        hidden layer could all be represented by a single matrix, or series of
        weights, between the input and output neurons. Don't worry if this
        doesn't quite make sense yet, for now let's just learn about the basics
        of linear algebra.
      </p>
      <p class="paragraph">
        For our purposes, only two aspects of linear algebra will really matter
        to us: vectors and matrices. A vector, in principle, is a very simple
        entity; all it does is denote a direction in space and a magnitude. In
        the coordinate system we are using these are not explicitly denoted by
        the vector itself, though both the magnitude and direction can be
        calculated with a trivial amount of effort. Here is an example of a very
        simple vector: $$ \vec{v} = \begin{bmatrix} 1\\ 2\\ \end{bmatrix} $$
        This vector has an x component of 1 and a y component of 2. Typically,
        vectors are centered at the origin, so in this case we can think of the
        vector with its base at \( (0, 0) \) and tip at \( (1, 2) \).
      </p>

      <br />

      <div class="image-container">
        <object data="/media/images/simple-vector.svg" type=""></object>
      </div>

      <br />

      <p class="paragraph">
        Now vectors can have any number of components, but the most we can
        visualize is 3. What make vectors interesting is that we can represent
        our inputs as vectors. While this explanation isn't completely
        necessary, it gives us a more geometric interpretation of how inputs can
        be transformed by the weights and biases. For our purposes, all a neural
        network does is transform some input vector into a space that can be
        easily classified. In our case, the perceptron was just an algorithm
        which took a 2 dimensional input vector and transformed it to a single
        dimensional output vector (which is just a vector lying on a number
        line) and applied a threshold to its own activation to determine whether
        it should activate. In linear algebra, we have a really useful tool to
        describe these transformations: matrices.
      </p>

      <p class="paragraph">
        Before we get on to how matrices work, we must first understand how
        vectors are, in a way, implicitly expressed. And before we understand
        that, we must first understand scalars and vector addition. Vector
        addition is very simple in principle: you simply add each component of
        each vector. This has the effect of first walking along the first vector
        from the origin and then, from the tip of that initial vector, walking
        along the second vector. This is what 3Blue1Brown calls "tip to tail."
        To show this, let us first define two vectors: $$ \vec{v} =
        \begin{bmatrix} 1\\ 2\\ \end{bmatrix}, \hspace{10px} \vec{u} =
        \begin{bmatrix} -5\\ 1\\ \end{bmatrix} $$ Add then the operation: $$
        \vec{v} + \vec{u} = \vec{w} $$ And to visualize this operation:
      </p>

      <br />

      <div class="image-container">
        <object data="/media/images/vector-addition.svg" type=""></object>
      </div>

      <br />

      <p class="paragraph">
        As I stated earlier, we are simply adding each component of the vector.
        Vector addition is just a way to record the effect of multiple vectors.
        Just as you would expect from simple single-number addition, vector
        addition is commutative meaning that the order in which the vectors are
        added together does not matter.
      </p>

      <br />

      <div class="image-container">
        <object
          data="/media/images/commutative-vector-addition.svg"
          type=""
        ></object>
      </div>

      <br />

      <p class="paragraph">
        Similar to vector addition, scalars work in a way you would probably
        expect. Scalars, as the name implies, simply scale the vector. This
        means that the vector will retain the same direction (which is slope in
        this coordinate system), but the magnitude will change. For example, a
        scalar operation on vector \( \vec{v} \) would look like $$ \vec{u} =
        2\vec{v} $$ and if we were to expand that out a bit, it would look like
        $$ \vec{u} = 2\begin{bmatrix} 1\\ 2\\ \end{bmatrix} $$ Now all we do is
        scale each component by the scalar $$ \vec{u} = \begin{bmatrix} 2 \cdot
        1\\ 2 \cdot 2\\ \end{bmatrix} $$ And there we go, now we have our scaled
        vector. If we were to plot both \( \vec{u} \) and \( \vec{v} \), it
        would look something like
      </p>

      <br />

      <div class="image-container">
        <object data="/media/images/scaled-vector.svg" type=""></object>
      </div>

      <br />

      <p class="paragraph">
        With that out of the way, we can now get on to my two favorite vectors,
        \( \hat{\imath} \) and \( \hat{\jmath} \). On paper, these two vectors
        are very simple $$ \hat{\imath} = \begin{bmatrix} 1\\ 0\\ \end{bmatrix},
        \hspace{15px} \hat{\jmath} = \begin{bmatrix} 0\\ 1\\ \end{bmatrix} $$
        but don't let their simplicity fool you, they have a very special
        property: they act as the set of basis vectors that all 2d vectors can
        be implicitly described as.
      </p>

      <p class="paragraph">
        To understand what I mean, let's take a closer look at those two
        vectors. Notice that \( \hat{\imath} \) only has non-zero component in
        the x direction? This means if we treated the first component of \(
        \vec{v} \) as a scalar to \( \hat{\imath} \), we would get a vector that
        represents only the x-direction magnitude of \( \vec{v} \) since the
        other component of the vector is 0. The same logic applies to \(
        \hat{\jmath} \) where if you treat the y component of \( \vec{v} \) as a
        scalar to \( \hat{\jmath} \), you will get a vector representing only
        the y-direction magnitude of \( \vec{v} \). We could write this a little
        more formally and say $$ \vec{v}_x \hat{\imath} = \begin{bmatrix}
        \vec{v}_x\\ 0 \end{bmatrix}, \hspace{15px} \vec{v}_y \hat{\jmath} =
        \begin{bmatrix} 0\\ \vec{v}_y \end{bmatrix}$$ To unpack this a little
        more, note that \( \vec{v}_x \) represents the x component of \( \vec{v}
        \) and \( \vec{v}_y \) represents the y component of \( \vec{v} \).
        These are just numbers, and in the first part of the expression they act
        as scalars and in the second part they act as the element of the vector.
        They are just numbers so you can do to they whatever you can do to a
        number. Now as an exercise to the reader, I encourage you to draw out
        the vectors \( \hat{\imath} \) and \( \hat{\jmath} \) as well as their
        scaled versions lined out in the previous expression as even though this
        is a fairly simple concept, I think this could really help build some
        intuition.
      </p>

      <p class="paragraph">
        Now let us take these scaled vectors and simply add them. Since both
        have elements that are "complementary" (as each of their non-zero
        element lines up with the other vector's zero component), by adding the
        vectors we simply get our original vector \( \vec{v} \). If we were to
        write out our whole expression and expand \( \hat{\imath} \) and \(
        \hat{\jmath} \) to see what I mean, it would look something like $$
        \vec{v}_x \begin{bmatrix} 1\\ 0 \end{bmatrix} + \vec{v}_y
        \begin{bmatrix} 0\\ 1 \end{bmatrix} = \begin{bmatrix} \vec{v}_x\\
        \vec{v}_y \end{bmatrix} = \vec{v}$$
      </p>

      <p class="paragraph">
        Now you are probably wondering, "Why should I care about this? All you
        did was get the sum of vectors that represent each component of the
        original vector which is just the original vector!" And yes, this
        example didn't show much, but we did something very powerful, something
        called a linear combination. In principle, a linear combination is
        fairly simple. In the case of vectors, all you do is get the sum of some
        scaled vectors. Earlier when I said that all vectors can be implicitly
        described using the unit vectors, what I mean was that any vector can be
        described as a linear combination of those unit vectors. If a set of
        vectors is able to describe any other vector in your current vector
        space, it is a valid basis vector. The reason we use \( \hat{\imath} \)
        and \( \hat{\jmath} \) as our basis vectors is that it's quite simple to
        construct your linear combinations because as each unit vector points a
        single unit in a single direction, and because it is just a single unit,
        each component of your vector can be used directly as the scalar for the
        corresponding unit vector. This doesn't mean that we can't use other
        vectors as our basis vectors however. To prove this, let's use the
        vectors \( \vec{v} \) and \( \vec{u} \) as our basis vectors where $$
        \vec{v} = \begin{bmatrix} 1\\ 2 \end{bmatrix}, \hspace{15px} \vec{u} =
        \begin{bmatrix} -1\\ 1 \end{bmatrix} $$ Let's also say that we want a
        linear combination of the vectors to form vector \( \vec{w} \) where $$
        \vec{w} = \begin{bmatrix} 4\\ 1 \end{bmatrix} $$ If we solve a basic
        system, we find that $$ \frac{5}{3} \vec{v} - \frac{7}{3} \vec{u} =
        \vec{w} $$ Which looks like
      </p>

      <br />

      <div class="image-container">
        <object data="/media/images/linear-combination.svg" type=""></object>
      </div>

      <br />

      <p class="paragraph">
        These basis vectors must satisfy an important condition: they must be
        linearly independent. This means that one vector cannot be described as
        the linear combination of the other vectors in the set. In our case, \(
        \vec{v} \) cannot be described as a linear combination of \( \vec{u} \)
        (you can have a linear combination of just one vector), and \( \vec{u}
        \) cannot be described as a linear combination of \( \vec{v} \). If a
        set of vectors lie upon a line, each of them are able to express the
        other if scaled, and this means they are linearly dependent. One example
        of two linearly dependent vectors could be $$ \vec{u} = \begin{bmatrix}
        1\\ 1 \end{bmatrix}, \hspace{15px} \vec{v} = \begin{bmatrix} -1\\ -1
        \end{bmatrix} $$ If we draw these out, we find they lie upon the same
        line and could be written as scaled version of each other.
      </p>

      <br />

      <div class="image-container">
        <object data="/media/images/linearly-dependent.svg" type=""></object>
      </div>

      <br />

      <p class="paragraph">
        Now think about how we could transform the space vectors are described
        in by changing our basis vectors. We wouldn't change the scalars applied
        to the basis vectors (since those scalars are the components of the
        vector we want to transform), but just the basis vectors themselves.
        This has the effect of performing some operation to the space, and
        subsequently transforming the vectors within that space.
      </p>

      <p class="paragraph">
        Think about ways we could do this. What if we wrote some array of
        typical vectors, but each vector in the array represents the new basis
        vector that would transform the space. This would act like where the new
        \( \hat{\imath} \) and \( \hat{\jmath} \) would land, and our vector can
        perform a linear combination with each of its components of the vector
        array against the transformed basis vector in the array that corresponds
        to that component, just as we would do with the typical \( \hat{\imath}
        \) and \( \hat{\jmath} \). If we wanted to see what the transformation
        does to a given vector, let's define a vector \( \vec{v} \) where $$
        \vec{v} = \begin{bmatrix} 1\\ 2 \end{bmatrix} $$ and describe the array
        of new basis vectors as $$ \begin{bmatrix} 1 & 2\\ 3 & 4 \end{bmatrix}
        $$ where each column represents where we want that column index's basis
        vector to land. Now we can perform a linear combination of these
        elements, this would be written as $$ \begin{bmatrix} 1 & 2\\ 3 & 4
        \end{bmatrix} \vec{v} = \begin{bmatrix} 1 & 2\\ 3 & 4 \end{bmatrix}
        \begin{bmatrix} 1\\ 2 \end{bmatrix} $$ and now we can perform the linear
        combination $$ 1\begin{bmatrix} 1\\ 3 \end{bmatrix} + 2\begin{bmatrix}
        2\\ 4 \end{bmatrix} = \begin{bmatrix} 5\\ 11 \end{bmatrix} $$
      </p>

      <br />

      <p class="paragraph">
        The eagle-eyed among you may have noticed that we just invented
        matrices! Matrices are the language in which we describe transformations
        of space. In fact, if we want a matrix that simply describes the linear
        combination of \( \hat{\imath} \) and \( \hat{\jmath} \), we can write
        the matrix as so $$ \begin{bmatrix} 1 & 0\\ 0 & 1\end{bmatrix} $$ This
        is known as the identity matrix, and is the almost implicit
        transformation that vectors undergo to be described in space.
      </p>

      <p class="paragraph">
        To put this all a little more succinctly, each component of a vector
        represents a scalar to the corresponding unit vector. The linear
        combination of all of the units vectors and the components of the vector
        describe the vector, and if we get the linear combination of the
        components of the vector and a different set of vectors (rather than the
        unit vectors), the new resulting will be the original vector as though
        it's basis vectors were different. This can sometimes be a little
        difficult to fully wrap one's head around, so I strongly recommend
        checking out other resources like
        <a
          href="https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab"
          class="hyperlink"
          >3Blue1Brown's course on linear algebra</a
        >
        to help fully understand this process as the purpose of this article is
        not to teach linear algebra.
      </p>

      <p class="paragraph">
        Now remember how I said this was somehow relevant to neural networks? I
        know that was a while ago (I think the explanation was worth it
        however), but we must not forget that this process is essentially what
        neural networks undergo when classifying data. Each layer of neurons
        represents some transformation to the input space, and we most often use
        matrices as the language to describe that transformation. For example,
        in our little perceptron, our function to determine whether we should
        play the piccolo was like a 1x2 matrix. Now when matrices are not
        square, that means they are doing one of two things: either describing
        the input vector in a higher dimension or describing the input vector in
        a lower dimension. In our case, we took two dimensions of input
        (practice time and enjoyment) and transformed it into a single
        dimensions output that we ended up applying a threshold two. If we
        thought about this a bit more generally, we could say that the
        corresponding output vector will contain the same amount of rows as the
        matrix (since the linear combination involves addition), and the columns
        represent the elements of the vector you are applying the combination to
        (since each column represents a vector to be scaled by the corresponding
        component in the vector you are multiplying the matrix to). For our
        piccolo example, the entire process can be visually (each of the black
        vectors represent example inputs)
      </p>

      <br />

      <div class="image-container">
        <object
          data="/media/video/perceptron-transformation.gif"
          type=""
          width="360px"
          height="280px"
        ></object>
      </div>

      <br />

      <p class="paragraph">
        As you can see (and as I said earlier), this matrix transformed the
        input into a single dimensional output. The expression \( x - y > 0 \)
        that we use to determine the relationship between enjoyment and practice
        time can be modeled as the matrix \( \begin{bmatrix} 1 & -1\end{bmatrix}
        \) (bias is not included since in linear algebra the origin must remain
        static throughout the transformation). Each column of this matrix
        represents the relationship that input has to the output. Think of each
        column as the set of weights that applies to a single input for all
        neurons in a layer. This is essentially how much the activation of a
        single input neuron should affect all output neurons. Now each row is
        the weights that apply to all input for a single neuron. This is how
        much all inputs should determine a single output neuron. This means our
        output is represented as a vector where each component of that vector
        represents the activation of a neuron in that layer. I encourage you to
        take a minute and apply this logic to the matrix we used for the piccolo
        example and apply this principle in general.
      </p>

      <p class="paragraph">
        This is essentially the process of each layer of a neural network. Each
        layer (besides the input layer, of course) will take the output of the
        preceding layer of neurons and transform it in some way, and then send
        it on as the input to the next layer of neurons for further processing.
        This process of repeating transforming the data is known as
        Feed-Forward. Here is an example of a neural network with two hidden
        layers with four neurons each
      </p>

      <div class="image-container">
        <object
          data="/media/images/neural-network-vector-graphic.svg"
          type=""
        ></object>
      </div>

      <br />

      <p class="paragraph">
        Now isn't this beautiful? We are now able to describe a method in which
        input space can be molded in order to effectively classify data. There
        is one problem with this approach however, our operations can only mold
        the space in a linear manner. There is no way we could, for example,
        classify points on whether they are inside or outside of a circle and we
        have no guarantees that a model architecture like this could engage in
        more complex and abstract classification like in the case of the MNIST
        dataset. This forces us to find a way to transform and warp space in a
        manner that can represent any decision boundary shape. To find out how
        we can do that, we must look at the reason why neural networks are so
        powerful in the first place, called
      </p>

      <br />

      <h2>The Universal Approximation Theorem</h2>

      <p class="paragraph">
        The universal approximation theorem is the reason why neural networks
        are so powerful. To put it simply, the universal approximation theorem
        states any neural network with a non-linear activation function can
        approximate any continuous function with an arbitrary amount of error.
        To put it a touch more mathematically, let's say we have a non-linear
        function \( f(x) \) and we would like to approximated that function with
        a neural network. Let's call this approximated function \( \hat{f}(x)
        \). The theorem states that for any \( \epsilon > 0 \), there exists a
        set of weights, biases, and layer size in a neural network with a single
        hidden layer where \( |f(x) - \hat{f}(x)| < \epsilon \). As you could
        imagine, this is an extremely powerful piece of mathematics. To use our
        future MNIST interpreter as an example, we could imagine some continuous
        function that takes in the pixel values as a vector an can perfectly
        classify it every time. Whether this function actually exists is
        irrelevant, what matters is that we can create some neural network that
        can approximate it to an error close to that of individual humans.
      </p>

      <p class="paragraph">
        Now there are some caveats to this theorem. Just because there
        <em>exists</em> a set of parameters that can approximate the function
        doesn't mean that we have a way to figure out what those parameters are.
        In later sections of this article we will go over how we can find a set
        of parameters that can find a local minimum, but that will almost
        certainly not be the best set of parameters for our network.
      </p>

      <br />
    </div>
    <div class="bottom"></div>
  </body>
</html>
